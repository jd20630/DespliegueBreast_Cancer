 -*- coding: utf-8 -*-
"""Interface_Breast_Cancer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AKxYzGsFIn6a4zNR4Mz6IVTT-8Xttx8B
"""

# Commented out IPython magic to ensure Python compatibility.
%%writefile app.py
import streamlit as st
import pandas as pd
import joblib
import os

st.title('Breast Cancer Prediction App')

uploaded_file = st.file_uploader("Upload your input Excel file", type=["xlsx"])

if uploaded_file is not None:
    try:
        df = pd.read_excel(uploaded_file)
        st.write("Uploaded Data:")
        st.dataframe(df)

        # Data Cleaning (removing 'id' and 'diagnosis')
        if 'id' in df.columns and 'diagnosis' in df.columns:
            df_processed = df.drop(['id', 'diagnosis'], axis=1)
            st.write("Data after removing 'id' and 'diagnosis':")
            st.dataframe(df_processed)
        elif 'id' in df.columns:
             df_processed = df.drop(['id'], axis=1)
             st.write("Data after removing 'id':")
             st.dataframe(df_processed)
        elif 'diagnosis' in df.columns:
             df_processed = df.drop(['diagnosis'], axis=1)
             st.write("Data after removing 'diagnosis':")
             st.dataframe(df_processed)
        else:
             df_processed = df.copy()
             st.write("No 'id' or 'diagnosis' columns to remove.")
             st.dataframe(df_processed)


        # Select relevant columns
        columns_to_keep = [
            'concave points_worst',
            'perimeter_se',
            'perimeter_worst',
            'radius_mean',
            'radius_se',
            'texture_mean',
            'texture_worst'
        ]

        # Check if all columns to keep are in the uploaded data
        missing_columns = [col for col in columns_to_keep if col not in df_processed.columns]
        if missing_columns:
            st.error(f"The uploaded file is missing the following required columns: {', '.join(missing_columns)}")
        else:
            df_filtered = df_processed[columns_to_keep]
            st.write("Data with selected features:")
            st.dataframe(df_filtered)

            # Load the scaler and model
            try:
                scaler = joblib.load('standard_scaler.joblib')
                model = joblib.load('best_bagging_mlp_model.joblib')
                label_mapping = joblib.load('diagnosis_label_mapping.joblib')

                # Apply scaling
                df_scaled = scaler.transform(df_filtered)
                df_scaled = pd.DataFrame(df_scaled, columns=df_filtered.columns)
                st.write("Scaled Data:")
                st.dataframe(df_scaled)

                # Make predictions
                predictions = model.predict(df_scaled)

                # Map predictions back to original labels
                reverse_label_mapping = {v: k for k, v in label_mapping.items()}
                predicted_labels = [reverse_label_mapping[int(pred)] for pred in predictions.tolist()]

                st.write("Predicted Labels:")
                st.write(predicted_labels)

            except FileNotFoundError:
                st.error("Scaler, model, or label mapping file not found. Please ensure 'standard_scaler.joblib', 'best_bagging_mlp_model.joblib', and 'diagnosis_label_mapping.joblib' are in the correct directory.")
            except Exception as e:
                st.error(f"An error occurred during prediction: {e}")

    except Exception as e:
        st.error(f"An error occurred while reading the Excel file: {e}")
